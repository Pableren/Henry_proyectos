{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "# instalar en el ambiente sklearn y pushear a github\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sistema de Recomendacion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la recomendacion, se utilizara un sistema de recomendación basado en el contenido para identificar ítems similares entre si, mediante el seno-coseno mas proximo:\n",
    "\n",
    "- Representación vectorial: Se representa cada usuario o ítem como un vector de características numéricas.\n",
    "\n",
    "Estas características pueden ser atributos explícitos (como género de película) o características implícitas derivadas del análisis de datos de interacción (palabras clave extraídas de reseñas).\n",
    "\n",
    "- Cálculo de similitud coseno: La similitud coseno entre dos vectores se calcula como el coseno del ángulo entre ellos. El coseno del ángulo varía entre -1 y 1, donde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/df_movies_parquet.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['belongs_to_collection'] = df['belongs_to_collection'].apply(json.loads)\n",
    "df['genres'] = df['genres'].apply(json.loads)\n",
    "df['production_companies'] = df['production_companies'].apply(json.loads)\n",
    "df['production_countries'] = df['production_countries'].apply(json.loads)\n",
    "df['spoken_languages'] = df['spoken_languages'].apply(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminacion de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0,subset=['overview'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf (del inglés Term frequency – Inverse document frequency), frecuencia de término – frecuencia inversa de documento (o sea, la frecuencia de ocurrencia del término en la colección de documentos), es una medida numérica que expresa cuán relevante es una palabra para un documento en una colección."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcion recomendar: recomienda 5 peliculas basadas en el score de similitud\n",
    "aclaracion: pasar por parametro el nombre exacto de la pelicula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Las recomendaciones para Toy Story son: ': ['Jumanji', 'Grumpier Old Men', 'Waiting to Exhale', 'Father of the Bride Part II', 'Heat']}\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('popular')\n",
    "stopwords = nltk.corpus.stopwords.words('english') # Guardamos stopwords\n",
    "lemmatizer = WordNetLemmatizer() # invocamos el lematizador\n",
    "\n",
    "def preprocesamiento(texto):\n",
    "    tokens = nltk.word_tokenize(texto.lower())\n",
    "    # para cada token que cumple la condicion de ser una letra(isalpha()) y no estar dentro de las stopwords\n",
    "    # se aplica el lematizador y se guarda en tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['processed_overview'] = df['overview'].apply(preprocesamiento)\n",
    "tfidf = TfidfVectorizer(max_df=0.2,max_features=5,sublinear_tf=True)  # Ajustar min_df y max_df según sea necesario\n",
    "tfidf_matrix = tfidf.fit_transform(df['processed_overview'])\n",
    "\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "indices = pd.Series(df.index, index=df['title']).drop_duplicates()\n",
    "def recomendacion(titulo):\n",
    "    idx = indices[titulo]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx])) # retorna una lista con los scores similares\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) # ordena las peliculas de puntaje similar\n",
    "    sim_scores = sim_scores[1:6]  # Obtener los 5 más similares\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    lista_top = df['title'].iloc[movie_indices].tolist()\n",
    "    return {f'Las recomendaciones para {titulo} son: ':lista_top}\n",
    "b = recomendacion('Toy Story')\n",
    "#b = recomendacion('The Magnificent One')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total de letras sin lematizar:  14365729\n",
      "total de letras lematizadas:  9439527\n"
     ]
    }
   ],
   "source": [
    "total_letras_process = 0\n",
    "total_letras = 0\n",
    "for x in df['overview']:\n",
    "    total_letras = total_letras + len(x)\n",
    "print(\"total de letras sin lematizar: \",total_letras)\n",
    "for x in df['processed_overview']:\n",
    "    total_letras_process = total_letras_process + len(x)\n",
    "print(\"total de letras lematizadas: \",total_letras_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convertir listas a str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def juntar_listas(lista):\n",
    "  if isinstance(lista, (list)):\n",
    "    return ','.join([str(elemento[1]) for elemento in lista])\n",
    "df['generos'] = df['genres'].apply(juntar_listas)\n",
    "#df[\"generos\"] = df[\"generos\"].apply(lambda x: x.split(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### funcion para verificar si contiene un genero o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_genres(genres, genres_to_filter):\n",
    "    genres_list = [genre.strip() for genre in genres.split(',')]\n",
    "    return any(genre in genres_list for genre in genres_to_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recomendacion con genero y score de similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\AppData\\Local\\Temp\\ipykernel_1500\\1676335067.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtrado.drop_duplicates(subset=['title'],inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Las recomendaciones para the hills have eyes part ii son: ': ['bull durham',\n",
       "  'the pest',\n",
       "  'the naked man',\n",
       "  'bonnie and clyde',\n",
       "  'anaconda']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english') # Guardamos stopwords\n",
    "lemmatizer = WordNetLemmatizer() # invocamos el lematizador\n",
    "\n",
    "### Funcion para lematizar el texto\n",
    "def preprocesamiento(texto):\n",
    "    tokens = nltk.word_tokenize(texto.lower())\n",
    "    # para cada token que cumple la condicion de ser una letra(isalpha()) y no estar dentro de las stopwords\n",
    "    # se aplica el lematizador y se guarda en tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "### Funcion para filtrar los las peliculas con generos similares(debe coincidir al menos 1 genero)\n",
    "def contains_genres(genres, genres_to_filter):\n",
    "    genres_list = [genre.strip() for genre in genres.split(',')]\n",
    "    return any(genre in genres_list for genre in genres_to_filter)\n",
    "df['title'] = df['title'].str.lower()\n",
    "### Funcion para recomendar peliculas en base a la similitud de las palabras de sus descripciones y genero \n",
    "def recomendacion(titulo: str):\n",
    "    # Procresamiento y validacion de datos\n",
    "    titulo = str(titulo).strip().lower()\n",
    "    linea = df[df['title']== titulo]\n",
    "    generos = linea['generos']\n",
    "    generos_list = generos.str.split(',')#transformar el str en una lista\n",
    "    generos_list = list(generos_list.values)\n",
    "    df_filtrado = df[df['generos'].apply(lambda x: contains_genres(x, generos_list[0]))]\n",
    "    #filtrar el dataset por las peliculas que compartan 1 genero al menos y dropear duplicados\n",
    "    df_filtrado.drop_duplicates(subset=['title'],inplace=True)\n",
    "    # se resetea el indice porque creamos el dataframe filtrado por los generos\n",
    "    df_filtrado = df_filtrado.reset_index()\n",
    "    df_filtrado.drop(columns=['index'],inplace=True)\n",
    "    indices = pd.Series(df_filtrado.index,index=df_filtrado['title']) #Crear los indices del dataframe\n",
    "    # Procesamiento de texto\n",
    "    df_filtrado['processed_overview'] = df_filtrado['overview'].apply(preprocesamiento) #se tokeniza el texto\n",
    "    # creación de matriz TF-IDF\n",
    "    # max_df = 0.5 equivale a eliminar del modelo los términos que aparecen en más del 50% de los documentos o peliculas.\n",
    "    # es decir, mientras mas reduzca el parametro \"max_df\" mayor sera la\n",
    "    # importancia relativa de las palabras menos frecuentes\n",
    "    tfidf = TfidfVectorizer(max_df=0.5, max_features=100, sublinear_tf=True)\n",
    "    # por el hecho del costo computacional que genera esta vectorizacion de palabras se usara max_features=50\n",
    "    tfidf_matrix = tfidf.fit_transform(df_filtrado['processed_overview'])\n",
    "    # Calcular la similitud del coseno\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    idx = indices[titulo]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:6]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    lista_top = df['title'].iloc[movie_indices].tolist()\n",
    "    return {f'Las recomendaciones para {titulo} son: ':lista_top}\n",
    "#b = recomendacion(' diCkson experimental sound film ')\n",
    "b = recomendacion('the hills have eyes part ii')\n",
    "#b = recomendacion(titulo='TOY story')\n",
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
